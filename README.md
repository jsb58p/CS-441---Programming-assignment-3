# CS-441---Programming-assignment-3

## Short Reflection Document:

  I started off using Claude, but I ran into some errors that I was unable to resolve within the 5-10 responses that I had with Claude, so I switched to ChatGPT. After many hours of struggling to get ChatGPT to follow very basic instructions, I switched to CoPilot. After spending a while arguing with CoPilot about the literal contents of my prompt (which CoPilot seems to think either don't exist, or are less important than whatever it wants to do), I decided to switch back to Claude and just paste the original instructions in. Once again, Claude proved its superiority by taking the original instructions, completely unedited, and completing the entire programming task on its own. There were some errors at first, but Claude was able to identify the problems and fix them based off of the error messages. I had some issues determining how to interpret some of the instructions, specifically the examples provided and their use of quotes (') (or rather their lack of use in some cases). None of the LLMs seemed to have a good answer for that. They gave answers that not only contradicted the answers given by other LLMs, but would even give responses that contradicted themselves. The only thing I manually changed in the program besides missing parentheses was changing "let-values*" to "let*-values" to fix the syntax error (in the "run-tests" function,  *which is commented out in the assignment submission*).
  
  Claude specifically is helpful in the sense that it can take basic instructions and make a working program out of it. The other LLMs are okay for asking basic, common concepts, but they are incapable of following instructions well enough to generate a unique program based on specific, detailed instructions. Claude also has a hard time following specific instructions, but seems to have access to enough programs/programming concepts that are used in the projects in this class that it can generate these programs based off of the vague, high-level descriptions provided in the instructions. At least that's my interpretation of it. 

  I do not feel like using AI has helped me become a better programmer. That isn't to say that Claude hasn't made better programs for me to turn in than I would have made on my own. But I don't really think I learned much from it. Most of my time was spent just trying to get the LLMs to follow basic tasks, not even necessarily generating code. It really seemed that the best tactic was to just paste the original instructions into Claude. I have never seen any demonstrations of the types of projects done in this class, nor have I ever seen any demonstrations of how LLMs are used during my time at UMKC, so I really have no clue what additional skills I would need to use LLMs more effectively. All I know is that it was 99% frustration, 1% pasting the original instructions into Claude and getting a complete solution with no understanding of how it works.  

  The big picture that I'm taking away is that AI is going to be used as a crutch for instructors who don't want to teach and don't really care if the students learn anything useful. The AI is obviously less accurate and reliable than a motivated instructor who is knowledgeable in the subject they are teaching, and I don't even think it's particularly close. I would have much rather seen demonstrations and examples made by a real human being who can actually explain these things in a way that makes sense than by an AI that doesn't care whether it's right or wrong and will gleefully provide blatantly false information in many of its responses.

  My advice to future students is to just paste the instructions into Claude. If the program works and does what it's supposed to, then try to figure out how it works. If it doesn't work, don't waste too much time debugging it, because if the errors aren't extremely small (like a couple of missing parentheses) then most likely it is not even close to a working solution and it would be a waste of time to try to understand what it's doing. Basically, the LLM either fully understands the premise of the program based off of a relatively simple description,  or it doesn't understand it at all. Trying to give more detail to the LLM will just confuse it, and it often just ignores instructions. Trying to get the LLM to write one line of code or one function at a time and building it up into a program seems like a waste of time as the LLM is not reliable enough and even after telling it multiple times exactly what line of code to generate it will often make mistakes. If you have to give the LLM more than a handful of prompts, then it's probably best to just start over from scratch. 

  My advice to future faculty devising courses is that you should actually teach the things that the students are going to be required to do in the class. For example, if the students are going to be required to use LLMs, then you should teach them how to use LLMs by providing actual demonstrations in class. If they are going to be making Parsers and Interpreters, then you should probably provide demonstrations and examples of techniques for coming up with the solutions for these kinds of things. With that being said, I don't think LLMs should be relied on in any way unless it is specifically for a class that is meant to teach the use of LLMs.
